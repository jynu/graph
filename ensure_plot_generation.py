# AT THE END OF YOUR run_duckdb_benchmark() FUNCTION, MAKE SURE YOU HAVE:

def run_duckdb_benchmark():
    """Run comprehensive benchmark across all DuckDB retrieval methods."""
    
    # ... all your existing benchmark code ...
    
    # At the end of the function:
    
    # Performance Summary
    print(f"\n{'='*100}")
    print("ğŸ“Š METHOD PERFORMANCE SUMMARY")
    print(f"{'='*100}")
    
    for method_name, perf in method_performance.items():
        if perf['success_count'] > 0:
            avg_time = perf['total_time'] / len(queries)
            avg_tables = perf['total_tables'] / perf['success_count']
            success_rate = (perf['success_count'] / len(queries)) * 100
            
            method_type = "ğŸ¤– LLM" if any(x in method_name for x in ['GPT', 'Gemini', 'Azure']) else "âš¡ Local"
            print(f"{method_type} {method_name:25}: {success_rate:5.1f}% success | {avg_time:6.3f}s avg | {avg_tables:4.1f} avg tables")
    
    # *** CRITICAL: Export results and create visualizations ***
    print(f"\nğŸ“Š Exporting results and creating visualizations...")
    excel_filename = export_results(results)
    
    print(f"\n{'='*100}")
    print("âœ… Comprehensive DuckDB Benchmark completed successfully!")
    print(f"ğŸ“Š Processed {len(queries)} queries with {len(retrievers)} methods")
    print(f"ğŸ“ Results exported to {excel_filename}")
    print(f"ğŸ“Š Visualizations saved as PNG files")
    print(f"ğŸ¯ Key Insights:")
    print(f"   â€¢ Check Recall@5 for graph traversal methods")
    print(f"   â€¢ Check Precision@1 for ranking methods") 
    print(f"   â€¢ Coverage shows comprehensive discovery")
    print(f"{'='*100}")
    
    return results

# ALSO MAKE SURE YOUR MAIN FUNCTION CALLS THE BENCHMARK:

def main():
    """Enhanced main function for DuckDB benchmark with comprehensive accuracy evaluation."""
    print("ğŸš€ Enhanced DuckDB Table Retrieval Benchmark with Comprehensive Accuracy")
    print("=" * 80)
    
    # ... existing setup code ...
    
    try:
        # Run the enhanced benchmark
        results = run_duckdb_benchmark()  # This should create the plots
        
        if results:
            print("\nğŸ¯ Benchmark Summary:")
            print(f"   ğŸ“Š Processed {len(results)} queries")
            print(f"   ğŸ“ˆ Comprehensive accuracy analysis completed")
            print(f"   ğŸ“Š Visualizations generated")
            print(f"   ğŸ” Check PNG files for detailed plots")
            
    except Exception as e:
        print(f"âŒ Enhanced benchmark failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()